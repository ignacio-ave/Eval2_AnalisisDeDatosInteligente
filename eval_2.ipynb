{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b97f52",
   "metadata": {},
   "source": [
    "# utility.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9143635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Utility : auxiliars functions\n",
    "# Solo usa: pandas y numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers internos\n",
    "# ------------------------------------------------------------\n",
    "def _embed_indices(N, d, tau):\n",
    "    \"\"\"Matriz (M x d) de índices para embedding con retardo tau.\n",
    "       M = N - (d-1)*tau; si M<=0 retorna None.\"\"\"\n",
    "    M = int(N - (d - 1) * tau)\n",
    "    if M <= 0:\n",
    "        return None\n",
    "    base  = np.arange(M)[:, None]           # (M x 1)\n",
    "    steps = (np.arange(d)[None, :] * tau)   # (1 x d)\n",
    "    return base + steps                     # (M x d)\n",
    "\n",
    "def _shannon_entropy(p):\n",
    "    \"\"\"Entropía de Shannon (nats). Ignora p<=0 por convención.\"\"\"\n",
    "    p = p[p > 0]\n",
    "    if p.size == 0:\n",
    "        return 0.0\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "def _quantize_dispersion_minmax(x, c):\n",
    "    \"\"\"Min–max → [0,1]; cuantiza a 1..c con floor(c*Xi+0.5) y clamp.\"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    if x.size == 0:\n",
    "        return np.zeros(0, dtype=int)\n",
    "    xmin = np.nanmin(x); xmax = np.nanmax(x)\n",
    "    if (not np.isfinite(xmin)) or (not np.isfinite(xmax)) or xmax == xmin:\n",
    "        Xi = np.zeros_like(x)\n",
    "    else:\n",
    "        Xi = (x - xmin) / (xmax - xmin)\n",
    "    Yi = np.floor(c * Xi + 0.5).astype(int)  # cuantización\n",
    "    Yi[Yi < 1] = 1\n",
    "    Yi[Yi > c] = c\n",
    "    return Yi\n",
    "\n",
    "def _dispersion_histogram(x, d, tau, c):\n",
    "    \"\"\"\n",
    "    Histograma de patrones de dispersión (K=c^d bins) con codificación\n",
    "    base-c CONSISTENTE: pesos ascendentes [1, c, c^2, ...].\n",
    "    Devuelve (p, K) con p normalizado.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    if x.size == 0 or d < 1 or tau < 1 or c < 2:\n",
    "        return None, None\n",
    "\n",
    "    idx = _embed_indices(x.size, d, tau)\n",
    "    if idx is None:\n",
    "        return None, None\n",
    "\n",
    "    # 1) cuantización 1..c\n",
    "    Y = _quantize_dispersion_minmax(x, c)\n",
    "\n",
    "    # 2) embedding y 3) codificación base-c (Yi-1)\n",
    "    emb   = Y[idx]                             # (M x d) con valores 1..c\n",
    "    K     = int(c ** d)\n",
    "    cpows = (c ** np.arange(d)).astype(int)    # [1, c, c^2, ...]  (clave p/consistencia)\n",
    "    codes = ((emb - 1) * cpows).sum(axis=1)    # 0..K-1\n",
    "\n",
    "    # 4) histograma → p\n",
    "    counts = np.bincount(codes, minlength=K)\n",
    "    total  = counts.sum()\n",
    "    if total == 0:\n",
    "        return None, None\n",
    "    p = counts / total\n",
    "    return p.astype(float), K\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CResidual-Dispersion Entropy (CRDE) — normalizada [0,1]\n",
    "# Firma EXACTA de la plantilla\n",
    "# ------------------------------------------------------------\n",
    "def entropy_dispersion(x, d, tau, c):\n",
    "    \"\"\"\n",
    "    Dispersion Entropy (Shannon) normalizada en [0,1].\n",
    "    \"\"\"\n",
    "    # usa SIEMPRE el mismo histograma/codificación que el resto\n",
    "    p, K = _dispersion_histogram(x, d, tau, c)\n",
    "    if p is None:\n",
    "        return np.nan\n",
    "    H = _shannon_entropy(p)\n",
    "    \n",
    "    return float(H / np.log(K)) if K > 1 else np.nan\n",
    "\n",
    "def _entropy_dispersion_cr(x, d, tau, c):\n",
    "    \"\"\"\n",
    "    CR Dispersion Entropy (residual), normalizada en [0,1] (versión opcional).\n",
    "    \"\"\"\n",
    "    p, K = _dispersion_histogram(x, d, tau, c)\n",
    "    if p is None:\n",
    "        return np.nan\n",
    "    S = np.cumsum(p[::-1])[::-1]\n",
    "    Spos = S[S > 0]\n",
    "    H = float(-(Spos * np.log(Spos)).sum())\n",
    "    k = np.arange(K)\n",
    "    Sunif = (K - k) / K\n",
    "    Hmax = float(-(Sunif[Sunif > 0] * np.log(Sunif[Sunif > 0])).sum())\n",
    "    return float(H / Hmax) if Hmax > 0 else np.nan\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Permutation Entropy (PE) — normalizada [0,1]\n",
    "# \n",
    "# ------------------------------------------------------------\n",
    "def entropy_permuta(x, m, tau):\n",
    "    \"\"\"\n",
    "    Permutation Entropy normalizada en [0,1] sobre la serie x.\n",
    "    Pasos: embedding (m,tau) → patrón ordinal estable → histograma (m!) →\n",
    "           Shannon / log(m!)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    if x.size == 0 or m < 2 or tau < 1:\n",
    "        return np.nan\n",
    "\n",
    "    idx = _embed_indices(x.size, m, tau)\n",
    "    if idx is None:\n",
    "        return np.nan\n",
    "\n",
    "    emb = x[idx]                       # (M x m)\n",
    "    M   = emb.shape[0]\n",
    "    codes = np.empty(M, dtype=int)\n",
    "\n",
    "    # tie-break estable: por valores y luego por índice\n",
    "    idx_local = np.arange(m)\n",
    "    for i in range(M):\n",
    "        row  = emb[i]\n",
    "        perm = np.lexsort((idx_local, row))  # ascendente y estable\n",
    "\n",
    "        # Código tipo Lehmer (simple)\n",
    "        code = 0\n",
    "        for a in range(m):\n",
    "            smaller = 0\n",
    "            for b in range(a+1, m):\n",
    "                if perm[b] < perm[a]:\n",
    "                    smaller += 1\n",
    "            # multiplicador = (m-1-a)!\n",
    "            mult = 1\n",
    "            for t in range(2, m - a):\n",
    "                mult *= t\n",
    "            code += smaller * mult\n",
    "        codes[i] = code\n",
    "\n",
    "    # m! bins\n",
    "    K = 1\n",
    "    for t in range(2, m+1):\n",
    "        K *= t\n",
    "\n",
    "    counts = np.bincount(codes, minlength=K)\n",
    "    total  = counts.sum()\n",
    "    if total == 0:\n",
    "        return np.nan\n",
    "    p = counts / total\n",
    "\n",
    "    H = _shannon_entropy(p)\n",
    "    return float(H / np.log(K)) if K > 1 else np.nan\n",
    "\n",
    "def entropy_mde(x, d, tau, c, S_max):\n",
    "    \"\"\"\n",
    "    Multi-Scale Dispersion Entropy (MDE)\n",
    "    x: serie temporal (array 1D)\n",
    "    d: dimension embedding\n",
    "    tau: factor de retardo\n",
    "    c: número de símbolos\n",
    "    S_max: número máximo de escalas\n",
    "    Devuelve: array de MDE para escalas 1 a S_max\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    mde_values = np.zeros(S_max)\n",
    "\n",
    "    for scale in range(1, S_max + 1):\n",
    "        # Paso 1: Dividir en ventanas de tamaño scale\n",
    "        window_size = scale\n",
    "        num_windows = N // window_size\n",
    "\n",
    "        if num_windows == 0:\n",
    "            mde_values[scale-1] = np.nan\n",
    "            continue\n",
    "\n",
    "        # Calcular promedio de cada ventana\n",
    "        y = np.zeros(num_windows)\n",
    "        for j in range(num_windows):\n",
    "            start = j * window_size\n",
    "            end = (j + 1) * window_size\n",
    "            y[j] = np.mean(x[start:end])\n",
    "\n",
    "        # Paso 2: Calcular entropía de dispersión para y\n",
    "        mde_values[scale-1] = entropy_dispersion(y, d, tau, c)\n",
    "\n",
    "    return mde_values\n",
    "\n",
    "def entropy_emde(x, d, tau, c, S_max):\n",
    "    \"\"\"\n",
    "    Enhanced Multi-Scale Dispersion Entropy (eMDE)\n",
    "    x: serie temporal (array 1D)\n",
    "    d: dimension embedding\n",
    "    tau: factor de retardo\n",
    "    c: número de símbolos\n",
    "    S_max: número máximo de escalas\n",
    "    Devuelve: array de eMDE para escalas 1 a S_max\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    emde_values = np.zeros(S_max)\n",
    "\n",
    "    # Escala 1: entropía de la serie original\n",
    "    emde_values[0] = entropy_dispersion(x, d, tau, c)\n",
    "\n",
    "    for scale in range(2, S_max + 1):\n",
    "        # Paso 1: Generar sub-series\n",
    "        avg_entropy = 0.0\n",
    "        for k in range(1, scale + 1):\n",
    "            # Sub-serie u_k = x[k:N]\n",
    "            u_k = x[k-1:]  # Ajuste de índice para 0-based\n",
    "\n",
    "            # Paso 2: Segmentar sub-serie\n",
    "            window_size = scale\n",
    "            num_windows = len(u_k) // window_size\n",
    "\n",
    "            if num_windows == 0:\n",
    "                continue\n",
    "\n",
    "            z = np.zeros(num_windows)\n",
    "            for j in range(num_windows):\n",
    "                start = j * window_size\n",
    "                end = (j + 1) * window_size\n",
    "                z[j] = np.mean(u_k[start:end])\n",
    "\n",
    "            # Calcular entropía para esta sub-serie\n",
    "            E_k = entropy_dispersion(z, d, tau, c)\n",
    "            avg_entropy += E_k\n",
    "\n",
    "        # Promedio de entropías para esta escala\n",
    "        emde_values[scale-1] = avg_entropy / scale\n",
    "\n",
    "    return emde_values\n",
    "\n",
    "def entropy_mpe(x, m, tau, S_max):\n",
    "    \"\"\"\n",
    "    Multi-Scale Permutation Entropy (MPE)\n",
    "    x: serie temporal (array 1D)\n",
    "    m: dimension embedding\n",
    "    tau: factor de retardo\n",
    "    S_max: número máximo de escalas\n",
    "    Devuelve: array de MPE para escalas 1 a S_max\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    mpe_values = np.zeros(S_max)\n",
    "\n",
    "    for scale in range(1, S_max + 1):\n",
    "        # Paso 1: Dividir en ventanas de tamaño scale\n",
    "        window_size = scale\n",
    "        num_windows = N // window_size\n",
    "\n",
    "        if num_windows == 0:\n",
    "            mpe_values[scale-1] = np.nan\n",
    "            continue\n",
    "\n",
    "        # Calcular promedio de cada ventana\n",
    "        y = np.zeros(num_windows)\n",
    "        for j in range(num_windows):\n",
    "            start = j * window_size\n",
    "            end = (j + 1) * window_size\n",
    "            y[j] = np.mean(x[start:end])\n",
    "\n",
    "        # Paso 2: Calcular entropía de permutación para y\n",
    "        mpe_values[scale-1] = entropy_permuta(y, m, tau)\n",
    "\n",
    "    return mpe_values\n",
    "\n",
    "def entropy_empe(x, m, tau, S_max):\n",
    "    \"\"\"\n",
    "    Enhanced Multi-Scale Permutation Entropy (eMPE)\n",
    "    x: serie temporal (array 1D)\n",
    "    m: dimension embedding\n",
    "    tau: factor de retardo\n",
    "    S_max: número máximo de escalas\n",
    "    Devuelve: array de eMPE para escalas 1 a S_max\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    empe_values = np.zeros(S_max)\n",
    "\n",
    "    # Escala 1: entropía de la serie original\n",
    "    empe_values[0] = entropy_permuta(x, m, tau)\n",
    "\n",
    "    for scale in range(2, S_max + 1):\n",
    "        # Paso 1: Generar sub-series\n",
    "        avg_entropy = 0.0\n",
    "        for k in range(1, scale + 1):\n",
    "            # Sub-serie u_k = x[k:N]\n",
    "            u_k = x[k-1:]  # Ajuste de índice para 0-based\n",
    "\n",
    "            # Paso 2: Segmentar sub-serie\n",
    "            window_size = scale\n",
    "            num_windows = len(u_k) // window_size\n",
    "\n",
    "            if num_windows == 0:\n",
    "                continue\n",
    "\n",
    "            z = np.zeros(num_windows)\n",
    "            for j in range(num_windows):\n",
    "                start = j * window_size\n",
    "                end = (j + 1) * window_size\n",
    "                z[j] = np.mean(u_k[start:end])\n",
    "\n",
    "            # Calcular entropía para esta sub-serie\n",
    "            E_k = entropy_permuta(z, m, tau)\n",
    "            avg_entropy += E_k\n",
    "\n",
    "        # Promedio de entropías para esta escala\n",
    "        empe_values[scale-1] = avg_entropy / scale\n",
    "\n",
    "    return empe_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a911d1af",
   "metadata": {},
   "source": [
    "# ppr.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8be220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "REQUERIMIENTOS FUNCIONALES Y NO FUNCIONALES: ppr.py\n",
    "(Etapa #1: Pre-Procesamiento de la Data)\n",
    "================================================================================\n",
    "\n",
    "--- RF-001: cargar_datos ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Carga Class1.csv a Class4.csv (4-muestras de N=12000 valores).             |\n",
    "|                     | Notación: 𝑋 ∈ ℜ^(N,4), N=12000.                                            |\n",
    "| **Entradas**        | - Rutas a Class1.csv, Class2.csv, Class3.csv, Class4.csv.                   |\n",
    "| **Salidas**         | - Matriz 𝑋 ∈ ℜ^(12000,4).                                                  |\n",
    "| **Parámetros**      | - rutas: Lista de rutas a archivos CSV.                                     |\n",
    "| **Requerimientos NF**| [RNF-001] Validar 12000 filas por archivo.                                |\n",
    "|                     | [RNF-002] Validar ausencia de valores NaN.                                  |\n",
    "| **Ejemplo**         | X = cargar_datos([\"Class1.csv\", \"Class2.csv\", \"Class3.csv\", \"Class4.csv\"])  |\n",
    "| **Notas**           | - Usar pandas: pd.read_csv(..., header=None).                               |\n",
    "\n",
    "--- RF-002: aplicar_diferencia_finita ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Aplica diferencias finitas: 𝑥(n) = 𝑥(n) − 𝑥(n−1), n=2,...,N.                 |\n",
    "| **Entradas**        | - Matriz 𝑋 ∈ ℜ^(12000,4).                                                  |\n",
    "| **Salidas**         | - Matriz 𝑋_diff ∈ ℜ^(11999,4).                                             |\n",
    "| **Parámetros**      | - X: Matriz de tamaño (12000,4).                                           |\n",
    "| **Requerimientos NF**| [RNF-003] Usar np.diff(X, axis=0).                                         |\n",
    "|                     | [RNF-004] Validar salida con 11999 filas.                                  |\n",
    "| **Ejemplo**         | X_diff = aplicar_diferencia_finita(X)                                       |\n",
    "| **Notas**           | - La primera fila se pierde al aplicar la diferencia.                       |\n",
    "\n",
    "--- RF-003: segmentar_muestras ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Segmenta muestras con longitud 𝑙𝐹 ∈ {1000,1200,1600}.                        |\n",
    "|                     | Notación: 𝑥_i ∈ ℜ^(𝑙𝐹, 𝑛𝐹), 𝑛𝐹 = N/𝑙𝐹.                                        |\n",
    "| **Entradas**        | - Matriz 𝑋_diff ∈ ℜ^(11999,4).                                             |\n",
    "|                     | - 𝑙𝐹: Longitud del segmento.                                                 |\n",
    "| **Salidas**         | - Lista de matrices segmentadas.                                             |\n",
    "| **Parámetros**      | - X_diff: Matriz de tamaño (11999,4).                                       |\n",
    "|                     | - lF: Longitud del segmento (1000, 1200, o 1600).                            |\n",
    "| **Requerimientos NF**| [RNF-005] Validar que 11999 sea divisible por 𝑙𝐹.                           |\n",
    "|                     | [RNF-006] Segmentar por filas, sin solapamiento.                             |\n",
    "| **Ejemplo**         | segmentos = segmentar_muestras(X_diff, lF=1000)                              |\n",
    "| **Notas**           | - Usar np.reshape.                                                           |\n",
    "\n",
    "--- RF-004: calcular_entropias ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Calcula entropías multi-escala (MDE, eMDE, MPE, eMPE) por segmento.         |\n",
    "| **Entradas**        | - Lista de segmentos.                                                       |\n",
    "|                     | - 𝑑, 𝜏, 𝑐, 𝑆_max: Parámetros de entropía.                                    |\n",
    "| **Salidas**         | - DataFrame de entropías concatenadas (dClases.csv).                        |\n",
    "| **Parámetros**      | - segmentos: Lista de matrices segmentadas.                                 |\n",
    "|                     | - d: Dimensión embedding.                                                   |\n",
    "|                     | - tau: Factor de retardo.                                                   |\n",
    "|                     | - c: Número de símbolos.                                                    |\n",
    "|                     | - Smax: Número máximo de escalas.                                           |\n",
    "| **Requerimientos NF**| [RNF-007] Implementar fórmulas según slides del profesor.                  |\n",
    "|                     | [RNF-008] Validar parámetros enteros positivos.                             |\n",
    "| **Ejemplo**         | dClases = calcular_entropias(segmentos, d=2, tau=1, c=3, Smax=10)           |\n",
    "| **Notas**           | - Concatenar los 4 datasets de entropías.                                   |\n",
    "\n",
    "--- RF-005: generar_etiquetas ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Crea etiquetas binarias para 4 clases: (1 0 0 0), (0 1 0 0), etc.           |\n",
    "| **Entradas**        | - Número de muestras por clase.                                             |\n",
    "| **Salidas**         | - DataFrame de etiquetas (dLabel.csv).                                      |\n",
    "| **Parámetros**      | - n_muestras_por_clase: Número de muestras.                                 |\n",
    "| **Requerimientos NF**| [RNF-009] Validar coincidencia de filas con segmentos.                     |\n",
    "| **Ejemplo**         | dLabel = generar_etiquetas(n_muestras_por_clase=30000)                      |\n",
    "| **Notas**           | - Usar one-hot encoding.                                                    |\n",
    "\n",
    "--- RF-006: guardar_configuracion ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Guarda configuración en conf_ppr.csv (6 líneas).                            |\n",
    "| **Entradas**        | - tipo_entropia, lF, d, tau, c, Smax.                                       |\n",
    "| **Salidas**         | - Archivo conf_ppr.csv.                                                     |\n",
    "| **Parámetros**      | - tipo_entropia: 1-4 (MDE, eMDE, MPE, eMPE).                                |\n",
    "|                     | - lF, d, tau, c, Smax: Parámetros numéricos.                                |\n",
    "| **Requerimientos NF**| [RNF-010] Validar parámetros enteros positivos.                            |\n",
    "| **Ejemplo**         | guardar_configuracion(1, 1000, 2, 1, 3, 10)                                 |\n",
    "| **Notas**           | - Formato: 1 línea por parámetro.                                           |\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ppr.py - Preprocesamiento de datos para clasificación con entropías multi-escala\n",
    "\n",
    "Este módulo implementa las funciones necesarias para:\n",
    "1. Cargar y preprocesar datos de las 4 clases\n",
    "2. Calcular diferencias finitas\n",
    "3. Segmentar los datos\n",
    "4. Calcular entropías multi-escala (MDE, eMDE, MPE, eMPE)\n",
    "5. Generar etiquetas para clasificación\n",
    "\n",
    "El archivo está diseñado para trabajar con configuraciones externas y no contiene\n",
    "valores default fijos, permitiendo optimización de parámetros desde main.py\n",
    "\"\"\"\n",
    "# ppr.py\n",
    "\"\"\"\n",
    "ppr.py - Preprocesamiento de datos para clasificación con entropías multi-escala\n",
    "Este módulo implementa las funciones necesarias para:\n",
    "1. Cargar y preprocesar datos de las 4 clases\n",
    "2. Calcular diferencias finitas\n",
    "3. Segmentar los datos\n",
    "4. Calcular entropías multi-escala (MDE, eMDE, MPE, eMPE)\n",
    "5. Generar etiquetas para clasificación\n",
    "El archivo está diseñado para trabajar con configuraciones externas y no contiene\n",
    "valores default fijos, permitiendo optimización de parámetros desde main.py\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from utility import (entropy_dispersion, entropy_permuta,\n",
    "#                    entropy_mde, entropy_emde, entropy_mpe, entropy_empe)\n",
    "# Constantes de rutas\n",
    "DATA_DIR = \"data\"\n",
    "CONF_DIR = \"config\"\n",
    "CONF_TEMP = os.path.join(CONF_DIR, \"conf_temp.csv\")\n",
    "CONF_OPTIMO = os.path.join(CONF_DIR, \"conf_optimo.csv\")\n",
    "\n",
    "def cargar_datos(rutas_classes):\n",
    "    \"\"\"\n",
    "    Carga los datos de las 4 clases desde archivos CSV\n",
    "    Returns:\n",
    "        numpy.ndarray: Matriz X de tamaño (120000, 4)\n",
    "    \"\"\"\n",
    "    matrices = []\n",
    "    for ruta in rutas_classes:\n",
    "        print(f\"[INFO] Cargando archivo: {ruta}\")\n",
    "        df = pd.read_csv(ruta, header=None)\n",
    "        print(f\"[DEBUG] {ruta} → shape = {df.shape}\")\n",
    "        matriz = df.values.astype(float)\n",
    "        if matriz.ndim == 1:\n",
    "            matriz = matriz[:, None]\n",
    "        # Seleccionar solo una columna por clase (ej. columna 0)\n",
    "        matrices.append(matriz[:, [0]])\n",
    "\n",
    "    # Validación de dimensiones\n",
    "    for i, matriz in enumerate(matrices):\n",
    "        if matriz.shape[0] != 120000:\n",
    "            print(f\"[ERROR] Clase #{i+1} no tiene 120000 filas → tiene {matriz.shape[0]}\")\n",
    "        if np.any(~np.isfinite(matriz)):\n",
    "            print(f\"[ERROR] Clase #{i+1} contiene NaN o infinitos.\")\n",
    "\n",
    "    X = np.hstack(matrices)\n",
    "    print(f\"[INFO] X final → shape = {X.shape} (esperado: (120000, 4))\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def aplicar_diferencia_finita(X):\n",
    "    \"\"\"\n",
    "    Aplica diferencias finitas\n",
    "    Returns:\n",
    "        numpy.ndarray: X_diff ∈ ℜ^(119999, 4)\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] aplicando diferencia finita → X.shape = {X.shape}\")\n",
    "    if X.shape != (120000, 4):\n",
    "        print(f\"[ERROR] X no tiene forma esperada (120000, 4)\")\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    print(f\"[INFO] X_diff.shape = {X_diff.shape} (esperado: (119999, 4))\")\n",
    "    return X_diff\n",
    "\n",
    "\n",
    "def segmentar_muestras(X_diff, lF):\n",
    "    \"\"\"\n",
    "    Segmenta muestras\n",
    "    \"\"\"\n",
    "    filas = X_diff.shape[0]\n",
    "    print(f\"[INFO] segmentar_muestras: X_diff.shape = {X_diff.shape}, lF = {lF}\")\n",
    "    resto = filas % lF\n",
    "    print(f\"[INFO]  filas % lF = {filas} % {lF} = {resto}\")\n",
    "\n",
    "    if resto != 0:\n",
    "        print(f\"[WARN] {filas} no divisible por {lF}, se recortarán {resto} filas.\")\n",
    "        X_diff = X_diff[:filas - resto, :]\n",
    "\n",
    "    n_segmentos = X_diff.shape[0] // lF\n",
    "    print(f\"[INFO] Se crearán {n_segmentos} segmentos de tamaño {lF}\")\n",
    "    segmentos = []\n",
    "    for i in range(n_segmentos):\n",
    "        start = i * lF\n",
    "        end = (i + 1) * lF\n",
    "        segmentos.append(X_diff[start:end, :])\n",
    "    return segmentos\n",
    "\n",
    "\n",
    "def calcular_entropias(segmentos, tipo_entropia, d, tau, c, S_max):\n",
    "    \"\"\"\n",
    "    Calcula entropías multi-escala para los segmentos\n",
    "    Args:\n",
    "        segmentos (list): Lista de segmentos\n",
    "        tipo_entropia (int): 1=MDE, 2=eMDE, 3=MPE, 4=eMPE\n",
    "        d (int): Dimensión embedding\n",
    "        tau (int): Factor de retardo\n",
    "        c (int): Número de símbolos (solo para MDE/eMDE)\n",
    "        S_max (int): Número máximo de escalas\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame con las entropías calculadas\n",
    "    \"\"\"\n",
    "    # Validar parámetros (RNF-008)\n",
    "    if not all(isinstance(p, int) and p > 0 for p in [d, tau, c, S_max]):\n",
    "        raise ValueError(\"Todos los parámetros deben ser enteros positivos\")\n",
    "    features = []\n",
    "    # Función para calcular entropías según el tipo\n",
    "    def calcular_entropia_serie(serie):\n",
    "        if tipo_entropia == 1:\n",
    "            return entropy_mde(serie, d, tau, c, S_max)\n",
    "        elif tipo_entropia == 2:\n",
    "            return entropy_emde(serie, d, tau, c, S_max)\n",
    "        elif tipo_entropia == 3:\n",
    "            return entropy_mpe(serie, d, tau, S_max)\n",
    "        elif tipo_entropia == 4:\n",
    "            return entropy_empe(serie, d, tau, S_max)\n",
    "        else:\n",
    "            raise ValueError(\"tipo_entropia debe ser 1-4\")\n",
    "    # Procesar cada segmento y cada clase\n",
    "    for segmento in segmentos:\n",
    "        for clase in range(4):  # 4 clases\n",
    "            serie = segmento[:, clase]\n",
    "            entropias = calcular_entropia_serie(serie)\n",
    "            features.append(entropias)\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def generar_etiquetas(n_muestras_por_clase):\n",
    "    \"\"\"\n",
    "    Genera etiquetas binarias para las clases\n",
    "    Args:\n",
    "        n_muestras_por_clase (int): Número de muestras por clase\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame con las etiquetas\n",
    "    \"\"\"\n",
    "    etiquetas = []\n",
    "    for i in range(4):\n",
    "        etiquetas.extend([i] * n_muestras_por_clase)\n",
    "    # Convertir a one-hot encoding\n",
    "    one_hot = np.eye(4)[etiquetas]\n",
    "    return pd.DataFrame(one_hot)\n",
    "\n",
    "def guardar_configuracion(tipo_entropia, lF, d, tau, c, S_max):\n",
    "    \"\"\"\n",
    "    Guarda la configuración en conf_temp.csv\n",
    "    Args:\n",
    "        tipo_entropia (int): 1-4\n",
    "        lF (int): Longitud del segmento\n",
    "        d (int): Dimensión embedding\n",
    "        tau (int): Factor de retardo\n",
    "        c (int): Número de símbolos\n",
    "        S_max (int): Número máximo de escalas\n",
    "    \"\"\"\n",
    "    # Validar parámetros (RNF-010)\n",
    "    if not all(isinstance(p, int) and p > 0 for p in [tipo_entropia, lF, d, tau, c, S_max]):\n",
    "        raise ValueError(\"Todos los parámetros deben ser enteros positivos\")\n",
    "    if tipo_entropia not in [1, 2, 3, 4]:\n",
    "        raise ValueError(\"tipo_entropia debe ser 1-4\")\n",
    "    config = [tipo_entropia, lF, d, tau, c, S_max]\n",
    "    pd.DataFrame(config).to_csv(CONF_TEMP, index=False, header=False)\n",
    "\n",
    "def procesar_datos(tipo_entropia, lF, d, tau, c, S_max):\n",
    "    print(\"\\n[ETAPA] Iniciando preprocesamiento completo...\\n\")\n",
    "    rutas = [os.path.join(DATA_DIR, f\"Class{i+1}.csv\") for i in range(4)]\n",
    "    X = cargar_datos(rutas)\n",
    "    X_diff = aplicar_diferencia_finita(X)\n",
    "    segmentos = segmentar_muestras(X_diff, lF)\n",
    "    print(f\"[INFO] Total de segmentos generados = {len(segmentos)}\")\n",
    "    features = calcular_entropias(segmentos, tipo_entropia, d, tau, c, S_max)\n",
    "    n_muestras = len(segmentos) * 4\n",
    "    labels = generar_etiquetas(n_muestras // 4)\n",
    "    guardar_configuracion(tipo_entropia, lF, d, tau, c, S_max)\n",
    "    print(\"[INFO] Preprocesamiento finalizado exitosamente.\\n\")\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def guardar_resultados(features, labels):\n",
    "    \"\"\"\n",
    "    Guarda los resultados en los archivos correspondientes\n",
    "    Args:\n",
    "        features (pandas.DataFrame): DataFrame con características\n",
    "        labels (pandas.DataFrame): DataFrame con etiquetas\n",
    "    \"\"\"\n",
    "    features.to_csv(os.path.join(DATA_DIR, \"dClases.csv\"), index=False, header=False)\n",
    "    labels.to_csv(os.path.join(DATA_DIR, \"dLabel.csv\"), index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2126b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ppr():\n",
    "    # leer conf\n",
    "    conf = pd.read_csv(CONF_OPTIMO, header=None).values.flatten()\n",
    "    tipo_entropia, lF, d, tau, c, S_max = map(int, conf)\n",
    "    # procesar datos\n",
    "    features, labels = procesar_datos(tipo_entropia, lF, d, tau, c, S_max)\n",
    "    # guardar resultados\n",
    "    guardar_resultados(features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41dcd723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ETAPA] Iniciando preprocesamiento completo...\n",
      "\n",
      "[INFO] Cargando archivo: data/Class1.csv\n",
      "[DEBUG] data/Class1.csv → shape = (120000, 4)\n",
      "[INFO] Cargando archivo: data/Class2.csv\n",
      "[DEBUG] data/Class2.csv → shape = (120000, 4)\n",
      "[INFO] Cargando archivo: data/Class3.csv\n",
      "[DEBUG] data/Class3.csv → shape = (120000, 4)\n",
      "[INFO] Cargando archivo: data/Class4.csv\n",
      "[DEBUG] data/Class4.csv → shape = (120000, 4)\n",
      "[INFO] X final → shape = (120000, 4) (esperado: (120000, 4))\n",
      "[INFO] aplicando diferencia finita → X.shape = (120000, 4)\n",
      "[INFO] X_diff.shape = (119999, 4) (esperado: (119999, 4))\n",
      "[INFO] segmentar_muestras: X_diff.shape = (119999, 4), lF = 1200\n",
      "[INFO]  filas % lF = 119999 % 1200 = 1199\n",
      "[WARN] 119999 no divisible por 1200, se recortarán 1199 filas.\n",
      "[INFO] Se crearán 99 segmentos de tamaño 1200\n",
      "[INFO] Total de segmentos generados = 99\n",
      "[INFO] Preprocesamiento finalizado exitosamente.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ppr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aaed28",
   "metadata": {},
   "source": [
    "# trn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b0a84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "REQUERIMIENTOS FUNCIONALES Y NO FUNCIONALES: trn.py\n",
    "(Etapa #2: Algoritmo de Entrenamiento)\n",
    "================================================================================\n",
    "\n",
    "--- RF-001: cargar_datos ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Carga dClases.csv y dLabel.csv.                                             |\n",
    "| **Entradas**        | - Rutas a dClases.csv y dLabel.csv.                                         |\n",
    "| **Salidas**         | - Matrices 𝑋 (características) y 𝑦 (etiquetas).                             |\n",
    "| **Parámetros**      | - ruta_X: Ruta a dClases.csv.                                               |\n",
    "|                     | - ruta_y: Ruta a dLabel.csv.                                                |\n",
    "| **Requerimientos NF**| [RNF-001] Validar mismo número de filas en 𝑋 y 𝑦.                          |\n",
    "| **Ejemplo**         | X, y = cargar_datos(\"dClases.csv\", \"dLabel.csv\")                            |\n",
    "\n",
    "--- RF-002: reordenar_aleatoriamente ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Reordena aleatoriamente 𝑋 y 𝑦 sincronizadamente.                            |\n",
    "| **Entradas**        | - Matrices 𝑋 y 𝑦.                                                           |\n",
    "| **Salidas**         | - Matrices 𝑋_shuffled y 𝑦_shuffled.                                         |\n",
    "| **Parámetros**      | - seed: Semilla para reproducibilidad (ej. 42).                             |\n",
    "| **Requerimientos NF**| [RNF-002] Usar np.random.seed(42).                                         |\n",
    "|                     | [RNF-003] Validar orden sincronizado.                                       |\n",
    "| **Ejemplo**         | X_shuffled, y_shuffled = reordenar_aleatoriamente(X, y)                     |\n",
    "\n",
    "--- RF-003: normalizar_zscore ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Normaliza características: 𝑥 = (𝑥 − mean(𝑥)) / std(𝑥).                       |\n",
    "| **Entradas**        | - Matriz 𝑋.                                                                 |\n",
    "| **Salidas**         | - Matriz 𝑋_norm normalizada.                                                |\n",
    "| **Parámetros**      | - X: Matriz de características.                                             |\n",
    "| **Requerimientos NF**| [RNF-004] Validar ausencia de divisiones por cero.                         |\n",
    "| **Ejemplo**         | X_norm = normalizar_zscore(X)                                               |\n",
    "\n",
    "--- RF-004: dividir_train_test ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Divide datos en entrenamiento (65-80%) y prueba.                            |\n",
    "| **Entradas**        | - 𝑋_norm, 𝑦_shuffled, porcentaje 𝑝 (65 < 𝑝 < 81).                            |\n",
    "| **Salidas**         | - 𝑋_train, 𝑋_test, 𝑦_train, 𝑦_test.                                          |\n",
    "| **Parámetros**      | - p: Porcentaje para entrenamiento (ej. 0.75).                              |\n",
    "| **Requerimientos NF**| [RNF-005] Validar 𝑝 ∈ [0.65, 0.80].                                        |\n",
    "| **Ejemplo**         | X_train, X_test, y_train, y_test = dividir_train_test(X_norm, y_shuffled, 0.75)|\n",
    "\n",
    "--- RF-005: entrenar_mgd ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Entrena modelo con mGD (descenso de gradiente con momentum).                |\n",
    "| **Entradas**        | - 𝑋_train, 𝑦_train, max_iter, 𝜇, momentum.                                  |\n",
    "| **Salidas**         | - Matriz de pesos 𝑊, vector de costo 𝐽.                                     |\n",
    "| **Parámetros**      | - max_iter: Máximo de iteraciones.                                          |\n",
    "|                     | - mu: Tasa de aprendizaje.                                                  |\n",
    "|                     | - momentum: Factor de momentum.                                             |\n",
    "| **Requerimientos NF**| [RNF-006] Inicializar 𝑊 aleatoriamente.                                    |\n",
    "|                     | [RNF-007] Validar disminución monótona del costo.                           |\n",
    "| **Ejemplo**         | W, J = entrenar_mgd(X_train, y_train, 1000, 0.01, 0.9)                      |\n",
    "\n",
    "--- RF-006: guardar_pesos_y_costo ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Guarda pesos y costos en pesos.csv y costo.csv.                             |\n",
    "| **Entradas**        | - 𝑊, 𝐽, rutas de salida.                                                    |\n",
    "| **Salidas**         | - Archivos pesos.csv y costo.csv.                                           |\n",
    "| **Parámetros**      | - ruta_pesos: Ruta a pesos.csv.                                             |\n",
    "|                     | - ruta_costo: Ruta a costo.csv.                                             |\n",
    "| **Requerimientos NF**| [RNF-008] Validar dimensiones de 𝑊 y 𝐽.                                    |\n",
    "| **Ejemplo**         | guardar_pesos_y_costo(W, J, \"pesos.csv\", \"costo.csv\")                       |\n",
    "\n",
    "--- RF-007: cargar_configuracion ---\n",
    "| Campo               | Descripción                                                                 |\n",
    "|---------------------|-----------------------------------------------------------------------------|\n",
    "| **Descripción**     | Carga configuración desde conf_train.csv.                                   |\n",
    "| **Entradas**        | - Ruta a conf_train.csv.                                                    |\n",
    "| **Salidas**         | - max_iter, 𝜇, 𝑝.                                                           |\n",
    "| **Parámetros**      | - ruta: Ruta al archivo.                                                    |\n",
    "| **Requerimientos NF**| [RNF-009] Validar valores numéricos positivos.                             |\n",
    "| **Ejemplo**         | max_iter, mu, p = cargar_configuracion(\"conf_train.csv\")                    |\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "trn.py - Algoritmo de Entrenamiento para Clasificación Multi-clases con Regresión Softmax\n",
    "\n",
    "Este módulo implementa las funciones necesarias para:\n",
    "1. Cargar y preprocesar datos de características y etiquetas\n",
    "2. Normalizar características usando Z-score\n",
    "3. Dividir datos en conjuntos de entrenamiento y prueba\n",
    "4. Entrenar modelo usando descenso de gradiente con momentum (mGD)\n",
    "5. Guardar pesos y costos del modelo\n",
    "\n",
    "El archivo está diseñado para trabajar con configuraciones externas y no contiene\n",
    "valores fijos, permitiendo optimización de parámetros desde main.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "# Constantes de rutas\n",
    "DATA_DIR = \"data\"\n",
    "CONF_DIR = \"config\"\n",
    "CONF_TRAIN = os.path.join(CONF_DIR, \"conf_train.csv\")\n",
    "\n",
    "def cargar_datos(ruta_X: str, ruta_y: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga datos de características y etiquetas desde archivos CSV\n",
    "\n",
    "    Args:\n",
    "        ruta_X: Ruta al archivo dClases.csv\n",
    "        ruta_y: Ruta al archivo dLabel.csv\n",
    "\n",
    "    Returns:\n",
    "        X: Matriz de características (N x D)\n",
    "        y: Matriz de etiquetas (N x K)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si las dimensiones no coinciden o hay valores no finitos\n",
    "    \"\"\"\n",
    "    X = pd.read_csv(ruta_X, header=None).values.astype(float)\n",
    "    y = pd.read_csv(ruta_y, header=None).values.astype(float)\n",
    "\n",
    "    # Validaciones (RNF-001)\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(f\"Número de filas diferente en X ({X.shape[0]}) e y ({y.shape[0]})\")\n",
    "    if not np.all(np.isfinite(X)):\n",
    "        raise ValueError(\"X contiene valores no finitos\")\n",
    "    if not np.all(np.isfinite(y)):\n",
    "        raise ValueError(\"y contiene valores no finitos\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def reordenar_aleatoriamente(X: np.ndarray, y: np.ndarray, seed: int = 42) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Reordena aleatoriamente X y y de manera sincronizada\n",
    "\n",
    "    Args:\n",
    "        X: Matriz de características\n",
    "        y: Matriz de etiquetas\n",
    "        seed: Semilla para reproducibilidad\n",
    "\n",
    "    Returns:\n",
    "        X_shuffled: Matriz de características reordenada\n",
    "        y_shuffled: Matriz de etiquetas reordenada\n",
    "    \"\"\"\n",
    "    # Validación (RNF-002, RNF-003)\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"X e y deben tener el mismo número de filas\")\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "def normalizar_zscore(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normaliza características usando Z-score: x = (x - mean(x)) / std(x)\n",
    "\n",
    "    Args:\n",
    "        X: Matriz de características\n",
    "\n",
    "    Returns:\n",
    "        X_norm: Matriz normalizada\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si hay división por cero\n",
    "    \"\"\"\n",
    "    # Validación (RNF-004)\n",
    "    if X.size == 0:\n",
    "        return X\n",
    "\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "\n",
    "    if np.any(std == 0):\n",
    "        raise ValueError(\"Desviación estándar cero detectada - no se puede normalizar\")\n",
    "\n",
    "    return (X - mean) / std\n",
    "\n",
    "def dividir_train_test(X_norm: np.ndarray, y_shuffled: np.ndarray, p: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Divide datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "    Args:\n",
    "        X_norm: Matriz de características normalizada\n",
    "        y_shuffled: Matriz de etiquetas reordenada\n",
    "        p: Porcentaje para entrenamiento (0.65 < p < 0.80)\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si p no está en el rango válido\n",
    "    \"\"\"\n",
    "    # Validación (RNF-005)\n",
    "    if not (0.65 <= p <= 0.80):\n",
    "        raise ValueError(\"p debe estar entre 0.65 y 0.80\")\n",
    "\n",
    "    N = X_norm.shape[0]\n",
    "    L = int(np.round(N * p))\n",
    "    L = max(1, min(L, N - 1))  # Asegurar al menos una muestra en cada conjunto\n",
    "\n",
    "    return X_norm[:L], X_norm[L:], y_shuffled[:L], y_shuffled[L:]\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcula la función softmax de manera numéricamente estable\n",
    "\n",
    "    Args:\n",
    "        z: Matriz de scores lineales\n",
    "\n",
    "    Returns:\n",
    "        Matriz de probabilidades\n",
    "    \"\"\"\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "def entrenar_mgd(X_train: np.ndarray, y_train: np.ndarray, max_iter: int, mu: float, beta: float = 0.9) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Entrena modelo usando descenso de gradiente con momentum (mGD)\n",
    "    \"\"\"\n",
    "    print(\"\\n[ETAPA] Entrenamiento con mGD\")\n",
    "    print(f\"[INFO] X_train.shape = {X_train.shape}\")\n",
    "    print(f\"[INFO] y_train.shape = {y_train.shape}\")\n",
    "    print(f\"[INFO] max_iter = {max_iter}, mu = {mu}, beta = {beta}\")\n",
    "\n",
    "    if X_train.shape[0] != y_train.shape[0]:\n",
    "        raise ValueError(\"X_train y y_train deben tener el mismo número de muestras\")\n",
    "\n",
    "    D = X_train.shape[1]\n",
    "    K = y_train.shape[1]\n",
    "\n",
    "    print(f\"[INFO] D (features) = {D}, K (clases) = {K}\")\n",
    "\n",
    "    # Inicializar pesos\n",
    "    W = np.random.randn(D, K) * 0.01  # W inicial sin bias\n",
    "    print(f\"[INFO] W inicial (sin bias) shape = {W.shape}\")\n",
    "\n",
    "    # Agregar bias a entrada y pesos\n",
    "    Xb = np.hstack([X_train, np.ones((X_train.shape[0], 1))])     # Bias como columna extra en X\n",
    "    W = np.vstack([W, np.zeros((1, K))])                           # Bias como fila extra en W\n",
    "    print(f\"[INFO] Xb shape (con bias) = {Xb.shape}\")\n",
    "    print(f\"[INFO] W shape (con bias) = {W.shape}\")\n",
    "\n",
    "    V = np.zeros_like(W)\n",
    "    J = np.zeros(max_iter)\n",
    "    eps = 1e-15\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        z = Xb @ W\n",
    "        yhat = softmax(z)\n",
    "\n",
    "        J[it] = -np.mean(np.sum(y_train * np.log(yhat + eps), axis=1))\n",
    "\n",
    "        grad = (Xb.T @ (yhat - y_train)) / X_train.shape[0]\n",
    "        V = beta * V + mu * grad\n",
    "        W = W - V\n",
    "        if it == 0 or (it + 1) % (max_iter // 5) == 0 or it == max_iter - 1:\n",
    "            preds = np.argmax(yhat, axis=1)\n",
    "            clases, counts = np.unique(preds, return_counts=True)\n",
    "            print(f\"[DEBUG] Predicciones por clase: {dict(zip(clases, counts))}\")\n",
    "    \n",
    "            print(f\"[INFO] Iteración {it+1}/{max_iter} → Costo J = {J[it]:.6f}\")\n",
    "\n",
    "    print(\"[ok] Entrenamiento finalizado.\\n\")\n",
    "    return W, J\n",
    "\n",
    "\n",
    "def guardar_pesos_y_costo(W: np.ndarray, J: np.ndarray, ruta_pesos: str, ruta_costo: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda pesos y costos en archivos CSV\n",
    "\n",
    "    Args:\n",
    "        W: Matriz de pesos\n",
    "        J: Vector de costos\n",
    "        ruta_pesos: Ruta para guardar pesos\n",
    "        ruta_costo: Ruta para guardar costos\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si las dimensiones no son válidas\n",
    "    \"\"\"\n",
    "    # Validación (RNF-008)\n",
    "    if W.ndim != 2:\n",
    "        raise ValueError(\"W debe ser una matriz 2D\")\n",
    "    if J.ndim != 1:\n",
    "        raise ValueError(\"J debe ser un vector 1D\")\n",
    "\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    pd.DataFrame(W).to_csv(ruta_pesos, index=False, header=False)\n",
    "    pd.DataFrame(J).to_csv(ruta_costo, index=False, header=False)\n",
    "\n",
    "def cargar_configuracion(ruta: str = CONF_TRAIN) -> Tuple[int, float, float]:\n",
    "    \"\"\"\n",
    "    Carga configuración desde archivo conf_train.csv\n",
    "\n",
    "    Args:\n",
    "        ruta: Ruta al archivo de configuración\n",
    "\n",
    "    Returns:\n",
    "        max_iter: Número máximo de iteraciones\n",
    "        mu: Tasa de aprendizaje\n",
    "        p: Porcentaje para entrenamiento\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si los valores no son válidos\n",
    "    \"\"\"\n",
    "    conf = pd.read_csv(ruta, header=None).values\n",
    "\n",
    "    # Validaciones (RNF-009)\n",
    "    max_iter = int(conf[0, 0])\n",
    "    mu = float(conf[1, 0])\n",
    "    p = float(conf[2, 0]) / 100.0 if conf[2, 0] > 1 else conf[2, 0]\n",
    "\n",
    "    if not (0 < mu < 1):\n",
    "        raise ValueError(\"mu debe estar entre 0 y 1\")\n",
    "    if not (0.65 <= p <= 0.80):\n",
    "        raise ValueError(\"p debe estar entre 0.65 y 0.80\")\n",
    "\n",
    "    return max_iter, mu, p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f0e59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trn():\n",
    "    \"\"\"\n",
    "    Función principal para ejecutar el proceso de entrenamiento\n",
    "    \"\"\"\n",
    "    # Cargar configuración\n",
    "    max_iter, mu, p = cargar_configuracion()\n",
    "\n",
    "    # Cargar datos\n",
    "    X, y = cargar_datos(os.path.join(DATA_DIR, \"dClases.csv\"), \n",
    "                       os.path.join(DATA_DIR, \"dLabel.csv\"))\n",
    "\n",
    "    # Reordenar aleatoriamente\n",
    "    X_shuffled, y_shuffled = reordenar_aleatoriamente(X, y)\n",
    "\n",
    "    # Normalizar\n",
    "    X_norm = normalizar_zscore(X_shuffled)\n",
    "\n",
    "    # Dividir en train/test\n",
    "    X_train, X_test, y_train, y_test = dividir_train_test(X_norm, y_shuffled, p)\n",
    "\n",
    "    # Guardar conjuntos de entrenamiento y prueba\n",
    "    pd.DataFrame(X_train).to_csv(os.path.join(DATA_DIR, \"dtrn.csv\"), index=False, header=False)\n",
    "    pd.DataFrame(y_train).to_csv(os.path.join(DATA_DIR, \"dtrn_label.csv\"), index=False, header=False)\n",
    "    pd.DataFrame(X_test).to_csv(os.path.join(DATA_DIR, \"dtst.csv\"), index=False, header=False)\n",
    "    pd.DataFrame(y_test).to_csv(os.path.join(DATA_DIR, \"dtst_label.csv\"), index=False, header=False)\n",
    "\n",
    "    # Entrenar modelo\n",
    "    W, J = entrenar_mgd(X_train, y_train, max_iter, mu)\n",
    "\n",
    "    # Guardar resultados\n",
    "    guardar_pesos_y_costo(W, J,\n",
    "                         os.path.join(DATA_DIR, \"pesos.csv\"),\n",
    "                         os.path.join(DATA_DIR, \"costo.csv\"))\n",
    "\n",
    "    print(\"[ok] Entrenamiento finalizado. Archivos guardados en data/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bdbafcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ETAPA] Entrenamiento con mGD\n",
      "[INFO] X_train.shape = (317, 10)\n",
      "[INFO] y_train.shape = (317, 4)\n",
      "[INFO] max_iter = 10000, mu = 0.2, beta = 0.9\n",
      "[INFO] D (features) = 10, K (clases) = 4\n",
      "[INFO] W inicial (sin bias) shape = (10, 4)\n",
      "[INFO] Xb shape (con bias) = (317, 11)\n",
      "[INFO] W shape (con bias) = (11, 4)\n",
      "[DEBUG] Predicciones por clase: {np.int64(0): np.int64(76), np.int64(1): np.int64(47), np.int64(2): np.int64(100), np.int64(3): np.int64(94)}\n",
      "[INFO] Iteración 1/10000 → Costo J = 1.386166\n",
      "[DEBUG] Predicciones por clase: {np.int64(0): np.int64(89), np.int64(1): np.int64(79), np.int64(2): np.int64(91), np.int64(3): np.int64(58)}\n",
      "[INFO] Iteración 2000/10000 → Costo J = 1.325039\n",
      "[DEBUG] Predicciones por clase: {np.int64(0): np.int64(89), np.int64(1): np.int64(79), np.int64(2): np.int64(91), np.int64(3): np.int64(58)}\n",
      "[INFO] Iteración 4000/10000 → Costo J = 1.325039\n",
      "[DEBUG] Predicciones por clase: {np.int64(0): np.int64(89), np.int64(1): np.int64(79), np.int64(2): np.int64(91), np.int64(3): np.int64(58)}\n",
      "[INFO] Iteración 6000/10000 → Costo J = 1.325039\n",
      "[DEBUG] Predicciones por clase: {np.int64(0): np.int64(89), np.int64(1): np.int64(79), np.int64(2): np.int64(91), np.int64(3): np.int64(58)}\n",
      "[INFO] Iteración 8000/10000 → Costo J = 1.325039\n",
      "[DEBUG] Predicciones por clase: {np.int64(0): np.int64(89), np.int64(1): np.int64(79), np.int64(2): np.int64(91), np.int64(3): np.int64(58)}\n",
      "[INFO] Iteración 10000/10000 → Costo J = 1.325039\n",
      "[ok] Entrenamiento finalizado.\n",
      "\n",
      "[ok] Entrenamiento finalizado. Archivos guardados en data/\n"
     ]
    }
   ],
   "source": [
    "trn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00392ca9",
   "metadata": {},
   "source": [
    "# test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for Softmax Regression\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "def cargar_datos(ruta_X_test, ruta_y_test, ruta_W):\n",
    "    print(\"[ETAPA] Cargando datos de prueba y pesos...\")\n",
    "    X_test = pd.read_csv(ruta_X_test, header=None).values.astype(float)\n",
    "    y_test = pd.read_csv(ruta_y_test, header=None).values.astype(float)\n",
    "    W = pd.read_csv(ruta_W, header=None).values.astype(float)\n",
    "\n",
    "    print(f\"[INFO] X_test shape = {X_test.shape}\")\n",
    "    print(f\"[INFO] y_test shape = {y_test.shape}\")\n",
    "    print(f\"[INFO] W shape = {W.shape}\")\n",
    "\n",
    "    if X_test.shape[0] != y_test.shape[0]:\n",
    "        raise ValueError(\"X_test y y_test deben tener el mismo número de filas\")\n",
    "    return X_test, y_test, W\n",
    "\n",
    "def predecir_softmax(X_test, W):\n",
    "    print(\"[ETAPA] Calculando probabilidades con softmax...\")\n",
    "    Xb = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    z = Xb @ W\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    y_pred_proba = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    return y_pred_proba\n",
    "\n",
    "def obtener_etiquetas_predichas(y_pred_proba):\n",
    "    return np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "def matriz_confusion(y_true, y_pred):\n",
    "    print(\"[ETAPA] Calculando matriz de confusión...\")\n",
    "    cm = np.zeros((4, 4), dtype=int)\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            cm[i, j] = np.sum((y_true == i) & (y_pred == j))\n",
    "    print(\"[INFO] Matriz de confusión:\\n\", cm)\n",
    "    return cm\n",
    "\n",
    "def calcular_fscores(cm):\n",
    "    print(\"[ETAPA] Calculando F-scores por clase...\")\n",
    "    fscores = np.zeros(4)\n",
    "    for i in range(4):\n",
    "        TP = cm[i, i]\n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        fscores[i] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        print(f\"[INFO] Clase {i}: Precision = {precision:.4f}, Recall = {recall:.4f}, F-score = {fscores[i]:.4f}\")\n",
    "    print(f\"[INFO] Macro F-score promedio = {np.mean(fscores):.4f}\")\n",
    "    return fscores\n",
    "\n",
    "def guardar_resultados(cm, fscores, ruta_cm, ruta_fs):\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    pd.DataFrame(cm).to_csv(ruta_cm, index=False, header=False)\n",
    "    pd.DataFrame(fscores).to_csv(ruta_fs, index=False, header=False)\n",
    "    print(f\"[ok] Resultados guardados en:\\n - {ruta_cm}\\n - {ruta_fs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "56ad6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    print(\"\\n========================\")\n",
    "    print(\"   EVALUACIÓN DEL MODELO\")\n",
    "    print(\"========================\\n\")\n",
    "\n",
    "    # Cargar datos\n",
    "    X_test, y_test, W = cargar_datos(\n",
    "        os.path.join(DATA_DIR, \"dtst.csv\"),\n",
    "        os.path.join(DATA_DIR, \"dtst_label.csv\"),\n",
    "        os.path.join(DATA_DIR, \"pesos.csv\")\n",
    "    )\n",
    "\n",
    "    # Convertir etiquetas one-hot a clases\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Predicción\n",
    "    y_pred_proba = predecir_softmax(X_test, W)\n",
    "    y_pred = obtener_etiquetas_predichas(y_pred_proba)\n",
    "\n",
    "    # Matriz de confusión y F-scores\n",
    "    cm = matriz_confusion(y_test_labels, y_pred)\n",
    "    fscores = calcular_fscores(cm)\n",
    "\n",
    "    # Guardar resultados\n",
    "    guardar_resultados(\n",
    "        cm, fscores,\n",
    "        os.path.join(DATA_DIR, \"cmatriz.csv\"),\n",
    "        os.path.join(DATA_DIR, \"fscores.csv\")\n",
    "    )\n",
    "\n",
    "    print(\"\\n[ok] Evaluación finalizada.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d570af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================\n",
      "   EVALUACIÓN DEL MODELO\n",
      "========================\n",
      "\n",
      "[ETAPA] Cargando datos de prueba y pesos...\n",
      "[INFO] X_test shape = (79, 10)\n",
      "[INFO] y_test shape = (79, 4)\n",
      "[INFO] W shape = (11, 4)\n",
      "[ETAPA] Calculando probabilidades con softmax...\n",
      "[ETAPA] Calculando matriz de confusión...\n",
      "[INFO] Matriz de confusión:\n",
      " [[6 4 4 4]\n",
      " [6 4 5 4]\n",
      " [6 3 5 6]\n",
      " [6 6 6 4]]\n",
      "[ETAPA] Calculando F-scores por clase...\n",
      "[INFO] Clase 0: Precision = 0.2500, Recall = 0.3333, F-score = 0.2857\n",
      "[INFO] Clase 1: Precision = 0.2353, Recall = 0.2105, F-score = 0.2222\n",
      "[INFO] Clase 2: Precision = 0.2500, Recall = 0.2500, F-score = 0.2500\n",
      "[INFO] Clase 3: Precision = 0.2222, Recall = 0.1818, F-score = 0.2000\n",
      "[INFO] Macro F-score promedio = 0.2395\n",
      "[ok] Resultados guardados en:\n",
      " - data/cmatriz.csv\n",
      " - data/fscores.csv\n",
      "\n",
      "[ok] Evaluación finalizada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21109f0",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
